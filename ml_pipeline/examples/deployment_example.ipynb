{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型部署示例\n",
    "\n",
    "本笔记本展示如何将训练好的TensorFlow模型转换为嵌入式系统可用的格式，包括：\n",
    "1. 模型加载和分析\n",
    "2. TensorFlow Lite转换\n",
    "3. 量化优化\n",
    "4. C代码生成\n",
    "5. 性能评估\n",
    "6. STM32部署准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 设置环境\n",
    "\n",
    "首先，我们需要导入必要的库并设置路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "\n",
    "# 添加父目录到路径\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('.'))))\n",
    "\n",
    "# 导入ML Pipeline模块\n",
    "from ml_pipeline.models.model import load_model_with_metadata\n",
    "from ml_pipeline.models.convert_model import (\n",
    "    convert_to_tflite, generate_c_array, analyze_model\n",
    ")\n",
    "from ml_pipeline.data_processing.data_processing import load_dataset\n",
    "\n",
    "# 设置绘图样式\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载模型\n",
    "\n",
    "加载训练好的模型和相关元数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model_path = 'trained_models/lstm_model.h5'\n",
    "model, metadata = load_model_with_metadata(model_path)\n",
    "\n",
    "# 显示模型信息\n",
    "print(\"模型信息:\")\n",
    "print(f\"架构: {metadata['architecture']}\")\n",
    "print(f\"输入形状: {metadata['input_shape']}\")\n",
    "print(f\"类别数: {metadata['num_classes']}\")\n",
    "print(f\"类别名称: {metadata['class_names']}\")\n",
    "print(f\"特征名称: {metadata['feature_names']}\")\n",
    "\n",
    "# 显示模型结构\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准备代表性数据集\n",
    "\n",
    "为量化校准准备代表性数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data_path = 'processed_data.npz'\n",
    "windows, labels, _ = load_dataset(data_path)\n",
    "\n",
    "# 创建代表性数据集生成器\n",
    "def representative_dataset():\n",
    "    \"\"\"生成代表性数据样本。\"\"\"\n",
    "    # 随机选择100个样本\n",
    "    indices = np.random.choice(len(windows), min(100, len(windows)), replace=False)\n",
    "    for idx in indices:\n",
    "        sample = windows[idx:idx+1].astype(np.float32)\n",
    "        yield [sample]\n",
    "\n",
    "# 显示数据集信息\n",
    "print(f\"数据集大小: {len(windows)} 个样本\")\n",
    "print(f\"样本形状: {windows.shape[1:]}\")\n",
    "\n",
    "# 显示一个样本\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i in range(3):\n",
    "    plt.plot(windows[0, :, i], label=metadata['feature_names'][i])\n",
    "plt.title('加速度数据示例')\n",
    "plt.xlabel('时间点')\n",
    "plt.ylabel('加速度')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(3, 6):\n",
    "    plt.plot(windows[0, :, i], label=metadata['feature_names'][i])\n",
    "plt.title('陀螺仪数据示例')\n",
    "plt.xlabel('时间点')\n",
    "plt.ylabel('角速度')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 转换为TFLite格式\n",
    "\n",
    "尝试不同的量化选项，比较它们的性能和大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建输出目录\n",
    "output_dir = 'embedded'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 定义要尝试的量化选项\n",
    "quantization_options = ['none', 'float16', 'int8', 'full_int8']\n",
    "model_sizes = {}\n",
    "conversion_results = {}\n",
    "\n",
    "# 转换模型\n",
    "for quantize in quantization_options:\n",
    "    print(f\"\\n转换模型 (量化: {quantize})...\")\n",
    "    \n",
    "    try:\n",
    "        # 转换模型\n",
    "        tflite_model = convert_to_tflite(\n",
    "            model=model,\n",
    "            quantize=quantize,\n",
    "            optimize=True,\n",
    "            representative_dataset=representative_dataset if quantize in ['int8', 'full_int8'] else None\n",
    "        )\n",
    "        \n",
    "        # 保存模型\n",
    "        tflite_path = os.path.join(output_dir, f\"model_{quantize}.tflite\")\n",
    "        with open(tflite_path, 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "        \n",
    "        # 记录模型大小\n",
    "        model_sizes[quantize] = len(tflite_model)\n",
    "        conversion_results[quantize] = 'success'\n",
    "        \n",
    "        print(f\"模型已保存到 {tflite_path}\")\n",
    "        print(f\"模型大小: {len(tflite_model) / 1024:.2f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"转换失败: {str(e)}\")\n",
    "        conversion_results[quantize] = 'failed'\n",
    "\n",
    "# 比较模型大小\n",
    "plt.figure(figsize=(10, 5))\n",
    "sizes = [size/1024 for size in model_sizes.values()]\n",
    "plt.bar(model_sizes.keys(), sizes)\n",
    "plt.title('不同量化选项的模型大小比较')\n",
    "plt.xlabel('量化方法')\n",
    "plt.ylabel('大小 (KB)')\n",
    "for i, size in enumerate(sizes):\n",
    "    plt.text(i, size, f'{size:.2f}KB', ha='center', va='bottom')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 生成C代码\n",
    "\n",
    "将选定的TFLite模型转换为C数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择最佳的量化模型\n",
    "best_quantize = 'float16'  # 可以根据实际需求选择\n",
    "tflite_path = os.path.join(output_dir, f\"model_{best_quantize}.tflite\")\n",
    "\n",
    "# 读取TFLite模型\n",
    "with open(tflite_path, 'rb') as f:\n",
    "    tflite_model = f.read()\n",
    "\n",
    "# 生成C代码\n",
    "c_code = generate_c_array(\n",
    "    tflite_model=tflite_model,\n",
    "    variable_name='g_model'\n",
    ")\n",
    "\n",
    "# 保存C文件\n",
    "c_path = os.path.join(output_dir, 'model.c')\n",
    "with open(c_path, 'w') as f:\n",
    "    f.write(c_code)\n",
    "\n",
    "# 生成头文件\n",
    "h_code = f\"\"\"#ifndef MODEL_DATA_H\n",
    "#define MODEL_DATA_H\n",
    "\n",
    "extern const unsigned char g_model[];\n",
    "extern const unsigned int g_model_len;\n",
    "\n",
    "#endif // MODEL_DATA_H\n",
    "\"\"\"\n",
    "\n",
    "h_path = os.path.join(output_dir, 'model.h')\n",
    "with open(h_path, 'w') as f:\n",
    "    f.write(h_code)\n",
    "\n",
    "print(f\"C文件已保存到 {c_path}\")\n",
    "print(f\"头文件已保存到 {h_path}\")\n",
    "\n",
    "# 显示C代码的前几行\n",
    "print(\"\\nC代码预览:\")\n",
    "print(\"===========\")\n",
    "print('\\n'.join(c_code.split('\\n')[:10]) + '\\n...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 性能分析\n",
    "\n",
    "分析转换后模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析模型\n",
    "results = analyze_model(\n",
    "    model_path=model_path,\n",
    "    tflite_path=tflite_path,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# 显示分析结果\n",
    "print(\"性能分析结果:\")\n",
    "print(\"=============\")\n",
    "print(f\"原始模型大小: {results['original_size'] / 1024:.2f} KB\")\n",
    "print(f\"TFLite模型大小: {results['tflite_size'] / 1024:.2f} KB\")\n",
    "print(f\"大小减少: {results['size_reduction'] * 100:.2f}%\")\n",
    "\n",
    "if 'benchmark' in results:\n",
    "    print(f\"\\n推理性能:\")\n",
    "    print(f\"平均推理时间: {results['benchmark']['average_time_ms']:.2f} ms\")\n",
    "    print(f\"每秒推理次数: {results['benchmark']['inference_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 验证转换后的模型\n",
    "\n",
    "比较原始模型和转换后模型的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载TFLite模型\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# 获取输入和输出细节\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"TFLite模型细节:\")\n",
    "print(\"输入:\")\n",
    "print(f\"  形状: {input_details[0]['shape']}\")\n",
    "print(f\"  类型: {input_details[0]['dtype']}\")\n",
    "print(\"输出:\")\n",
    "print(f\"  形状: {output_details[0]['shape']}\")\n",
    "print(f\"  类型: {output_details[0]['dtype']}\")\n",
    "\n",
    "# 选择一些测试样本\n",
    "n_samples = 100\n",
    "test_samples = windows[:n_samples]\n",
    "test_labels = labels[:n_samples]\n",
    "\n",
    "# 获取原始模型预测\n",
    "original_predictions = model.predict(test_samples)\n",
    "if original_predictions.shape[1] > 1:\n",
    "    original_predictions = np.argmax(original_predictions, axis=1)\n",
    "else:\n",
    "    original_predictions = (original_predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# 获取TFLite模型预测\n",
    "tflite_predictions = []\n",
    "for sample in test_samples:\n",
    "    # 设置输入张量\n",
    "    interpreter.set_tensor(input_details[0]['index'], np.array([sample], dtype=np.float32))\n",
    "    \n",
    "    # 运行推理\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # 获取输出\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    if output.shape[1] > 1:\n",
    "        pred = np.argmax(output)\n",
    "    else:\n",
    "        pred = (output > 0.5).astype(int)[0]\n",
    "    \n",
    "    tflite_predictions.append(pred)\n",
    "\n",
    "tflite_predictions = np.array(tflite_predictions)\n",
    "\n",
    "# 比较预测结果\n",
    "matches = (original_predictions == tflite_predictions)\n",
    "accuracy = np.mean(matches)\n",
    "\n",
    "print(f\"\\n预测比较:\")\n",
    "print(f\"预测匹配率: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# 显示不匹配的预测\n",
    "mismatches = np.where(~matches)[0]\n",
    "if len(mismatches) > 0:\n",
    "    print(\"\\n预测不匹配的样本:\")\n",
    "    for idx in mismatches[:5]:  # 显示前5个不匹配\n",
    "        print(f\"样本 {idx}:\")\n",
    "        print(f\"  真实标签: {metadata['class_names'][int(test_labels[idx])]}\")\n",
    "        print(f\"  原始预测: {metadata['class_names'][int(original_predictions[idx])]}\")\n",
    "        print(f\"  TFLite预测: {metadata['class_names'][int(tflite_predictions[idx])]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. STM32部署准备\n",
    "\n",
    "生成STM32项目所需的文件和配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建STM32项目目录\n",
    "stm32_dir = os.path.join(output_dir, 'stm32')\n",
    "os.makedirs(stm32_dir, exist_ok=True)\n",
    "\n",
    "# 生成预处理和后处理函数\n",
    "preprocess_code = f\"\"\"\n",
    "void preprocess_data(float* input_data, int length, int n_features) {{\n",
    "    // 标准化参数\n",
    "    const float means[6] = {{0.0f, 0.0f, 1.0f, 0.0f, 0.0f, 0.0f}};\n",
    "    const float stds[6] = {{1.0f, 1.0f, 1.0f, 1.0f, 1.0f, 1.0f}};\n",
    "    \n",
    "    // 对每个特征进行标准化\n",
    "    for (int i = 0; i < length; i++) {{\n",
    "        for (int j = 0; j < n_features; j++) {{\n",
    "            input_data[i * n_features + j] = \n",
    "                (input_data[i * n_features + j] - means[j]) / stds[j];\n",
    "        }}\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "postprocess_code = f\"\"\"\n",
    "int postprocess_output(float* output_data, int n_classes) {{\n",
    "    // 找到最大概率的类别\n",
    "    float max_prob = output_data[0];\n",
    "    int pred_class = 0;\n",
    "    \n",
    "    for (int i = 1; i < n_classes; i++) {{\n",
    "        if (output_data[i] > max_prob) {{\n",
    "            max_prob = output_data[i];\n",
    "            pred_class = i;\n",
    "        }}\n",
    "    }}\n",
    "    \n",
    "    return pred_class;\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# 生成主要处理函数\n",
    "main_code = f\"\"\"\n",
    "#include \"model.h\"\n",
    "#include \"tensorflow/lite/micro/all_ops_resolver.h\"\n",
    "#include \"tensorflow/lite/micro/micro_interpreter.h\"\n",
    "#include \"tensorflow/lite/micro/micro_mutable_op_resolver.h\"\n",
    "#include \"tensorflow/lite/schema/schema_generated.h\"\n",
    "\n",
    "// 全局变量\n",
    "constexpr int kTensorArenaSize = 32768;  // 根据需要调整\n",
    "uint8_t tensor_arena[kTensorArenaSize];\n",
    "\n",
    "// 模型接口\n",
    "tflite::MicroInterpreter* interpreter = nullptr;\n",
    "TfLiteTensor* input = nullptr;\n",
    "TfLiteTensor* output = nullptr;\n",
    "\n",
    "// 初始化函数\n",
    "bool initialize_model() {{\n",
    "    // 加载模型\n",
    "    const tflite::Model* model = tflite::GetModel(g_model);\n",
    "    if (model->version() != TFLITE_SCHEMA_VERSION) {{\n",
    "        return false;\n",
    "    }}\n",
    "    \n",
    "    // 创建操作解析器\n",
    "    static tflite::MicroMutableOpResolver<4> micro_op_resolver;\n",
    "    micro_op_resolver.AddFullyConnected();\n",
    "    micro_op_resolver.AddReshape();\n",
    "    micro_op_resolver.AddSoftmax();\n",
    "    \n",
    "    // 创建解释器\n",
    "    static tflite::MicroInterpreter static_interpreter(\n",
    "        model, micro_op_resolver, tensor_arena, kTensorArenaSize);\n",
    "    interpreter = &static_interpreter;\n",
    "    \n",
    "    // 分配张量\n",
    "    TfLiteStatus allocate_status = interpreter->AllocateTensors();\n",
    "    if (allocate_status != kTfLiteOk) {{\n",
    "        return false;\n",
    "    }}\n",
    "    \n",
    "    // 获取输入输出张量\n",
    "    input = interpreter->input(0);\n",
    "    output = interpreter->output(0);\n",
    "    \n",
    "    return true;\n",
    "}}\n",
    "\n",
    "// 预测函数\n",
    "int predict(float* data, int length, int n_features) {{\n",
    "    // 预处理\n",
    "    preprocess_data(data, length, n_features);\n",
    "    \n",
    "    // 复制数据到输入张量\n",
    "    for (int i = 0; i < length * n_features; i++) {{\n",
    "        input->data.f[i] = data[i];\n",
    "    }}\n",
    "    \n",
    "    // 运行推理\n",
    "    TfLiteStatus invoke_status = interpreter->Invoke();\n",
    "    if (invoke_status != kTfLiteOk) {{\n",
    "        return -1;\n",
    "    }}\n",
    "    \n",
    "    // 后处理\n",
    "    return postprocess_output(output->data.f, {metadata['num_classes']});\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# 保存文件\n",
    "with open(os.path.join(stm32_dir, 'preprocess.c'), 'w') as f:\n",
    "    f.write(preprocess_code)\n",
    "\n",
    "with open(os.path.join(stm32_dir, 'postprocess.c'), 'w') as f:\n",
    "    f.write(postprocess_code)\n",
    "\n",
    "with open(os.path.join(stm32_dir, 'model_interface.cpp'), 'w') as f:\n",
    "    f.write(main_code)\n",
    "\n",
    "# 复制模型文件\n",
    "import shutil\n",
    "shutil.copy2(os.path.join(output_dir, 'model.c'), os.path.join(stm32_dir, 'model.c'))\n",
    "shutil.copy2(os.path.join(output_dir, 'model.h'), os.path.join(stm32_dir, 'model.h'))\n",
    "\n",
    "print(\"STM32项目文件已生成:\")\n",
    "print(f\"  - {os.path.join(stm32_dir, 'preprocess.c')}\")\n",
    "print(f\"  - {os.path.join(stm32_dir, 'postprocess.c')}\")\n",
    "print(f\"  - {os.path.join(stm32_dir, 'model_interface.cpp')}\")\n",
    "print(f\"  - {os.path.join(stm32_dir, 'model.c')}\")\n",
    "print(f\"  - {os.path.join(stm32_dir, 'model.h')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 使用命令行工具\n",
    "\n",
    "展示如何使用命令行工具进行模型转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"模型转换命令示例:\")\n",
    "print(\"convert-model \\\n",
    "    --model trained_models/lstm_model.h5 \\\n",
    "    --output-dir embedded \\\n",
    "    --quantize float16 \\\n",
    "    --format both \\\n",
    "    --optimize \\\n",
    "    --representative-data processed_data.npz \\\n",
    "    --target-device stm32f4 \\\n",
    "    --analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 总结\n",
    "\n",
    "在本笔记本中，我们展示了如何将训练好的模型转换为嵌入式系统可用的格式：\n",
    "\n",
    "1. 加载和分析模型\n",
    "2. 准备代表性数据集\n",
    "3. 尝试不同的量化选项\n",
    "4. 生成C代码\n",
    "5. 分析模型性能\n",
    "6. 验证转换后的模型\n",
    "7. 准备STM32部署文件\n",
    "\n",
    "这个流程可以作为将机器学习模型部署到嵌入式系统的参考。通过调整量化参数和优化选项，你可以在模型大小和性能之间找到最佳平衡点。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}